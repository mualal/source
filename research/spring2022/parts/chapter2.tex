\chapter*{2. Обучение моделей} \label{ch2}
\addcontentsline{toc}{chapter}{2. Обучение моделей}
Во второй главе на основе рассматриваемых датасетов обучены модели машинного обучения и проведено сравнение диаграмм размаха ошибок построенных моделей с аналогичными диаграммами из работы \cite{muravtsev:metamodel}.

\section{Линейная регрессия} \label{ch1:sec1}
Из диаграмм размаха ошибок видим, что в случае линейной регрессии на некоторых кейсах ошибки достигают высоких значений. Это связано с ограниченной гибкостью линейной модели и небольшим количеством кейсов в области входных параметров, которой соответствуют низкие значения дебитов (менее 5 $\text{м}^3/\text{сут}$ за каждый месяц в течение всех рассматриваемых пяти лет).

\begin{figure}[H] 
	\center
	\includegraphics[width=\textwidth]{images/linear_regression_test_errors_boxplot_scenario1}
	\caption{Диаграмма ошибок прогноза линейной регрессии (сценарий 1)} 
	\label{fig:lin-reg-1}
\end{figure}

Прогнозы линейной регрессии в случае второго сценария разработки значительно хуже первого сценария (что видно из гораздо более длинных усов на диаграммах размаха ошибок в 5-25 месяцы).

\begin{figure}[H] 
	\center
	\includegraphics[width=\textwidth]{images/linear_regression_test_errors_boxplot_scenario2}
	\caption{Диаграмма ошибок прогноза линейной регрессии (сценарий 2)} 
	\label{fig:lin-reg-2}
\end{figure}


\section{Регрессия ближайших соседей} \label{ch1:sec2}

Видим, что прогнозы регрессии ближайших соседей значительно лучше (особенно в случае второго сценария разработки), чем прогнозы линейной регрессии. Но по-прежнему есть высокие значения ошибок вследствие несбалансированности данных (количество примеров с низкими значениями выходных дебитов в обучающей выборке существенно меньше, чем количество примеров со средними или высокими значениями дебитов).  

\begin{figure}[H] 
	\center
	\includegraphics[width=\textwidth]{images/nearest_neighs_approximation_test_errors_boxplot_scenario1}
	\caption{Диаграмма ошибок прогноза регрессии ближайших соседей (сценарий 1)} 
	\label{fig:neighs-reg-1}
\end{figure}

\begin{figure}[H] 
	\center
	\includegraphics[width=\textwidth]{images/nearest_neighs_approximation_test_errors_boxplot_scenario2}
	\caption{Диаграмма ошибок прогноза регрессии ближайших соседей (сценарий 2)} 
	\label{fig:neighs-reg-2}
\end{figure}


\section{Регрессия на основе метода опорных векторов} \label{ch1:sec3}

Видим, что регрессия опорных векторов даёт гораздо меньшие ошибки, чем линейная регрессия и регрессия ближайших соседей.

\begin{figure}[H] 
	\center
	\includegraphics[width=\textwidth]{images/svm_approximation_test_errors_boxplot_scenario1}
	\caption{Диаграмма ошибок прогноза регрессии опорных векторов (сценарий 1)} 
	\label{fig:svm-reg-1}
\end{figure}

\begin{figure}[H] 
	\center
	\includegraphics[width=\textwidth]{images/svm_approximation_test_errors_boxplot_scenario2}
	\caption{Диаграмма ошибок прогноза регрессии опорных векторов (сценарий 2)} 
	\label{fig:svm-reg-2}
\end{figure}

\section{Чрезвычайно рандомизированные деревья} \label{ch1:sec4}

Из диаграмм размаха ошибок видим, что регрессия на основе чрезвычайно рандомизированных деревьев может быть полезна при составлении прогноза для данных, подобных данным второго сценария разработки.

\begin{figure}[H] 
	\center
	\includegraphics[width=\textwidth]{images/extra_trees_regression_test_errors_boxplot_scenario1}
	\caption{Диаграмма ошибок прогноза регрессии на основе чрезвычайно рандомизированных деревьев (сценарий 1)} 
	\label{fig:extra-trees-reg-1}
\end{figure}

\begin{figure}[H] 
	\center
	\includegraphics[width=\textwidth]{images/extra_trees_regression_test_errors_boxplot_scenario2}
	\caption{Диаграмма ошибок прогноза регрессии на основе чрезвычайно рандомизированных деревьев (сценарий 2)} 
	\label{fig:extra-trees-reg-2}
\end{figure}

\section{Расширенные с помощью базисных функций линейные модели} \label{ch1:sec5}

\section{Выводы} \label{ch1:conclusion}

В текущей главе были построены аппроксимации разными методами машинного обучения. Наиболее точными оказались регрессия на основе метода опорных векторов и регрессия на основе чрезвычайно рандомизированных деревьев. Однако точность их прогноза не значительно лучше (а для некоторых кейсов хуже) точности прогноза нейронных сетей и градиентного бустинга на основе деревьев регрессии из работы \cite{muravtsev:metamodel}.




